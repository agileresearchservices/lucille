# This is a configuration file managed by https://github.com/lightbend/config
# It is in HOCON format, a superset of JSON

## Connector(s) Config: ------------------------------------------------------------------------------------------------------------
  connectors: [
      {class: "com.kmwllc.lucille.connector.CSVConnector",
       path: "/Volumes/Work/lucille/src/test/resources/test.csv", name: "connector1", pipeline: "pipeline1"},
      {class: "com.kmwllc.lucille.connector.CSVConnector",
       path: "/Volumes/Work/lucille/src/test/resources/test4.csv", name: "connector2", pipeline: "pipeline2"}
  ]
## ---------------------------------------------------------------------------------------------------------------------------------


## Runner Config: ---------------------------------------------------------------------------------------------------------------
  runner {
    # outputs detailed metrics to the main lucille log at the end of each run
    metricsLoggingLevel: "INFO"
    # sets the connector timeout (in ms), defaults to 86400000ms
    connectorTimeout: 100000
  }
## ---------------------------------------------------------------------------------------------------------------------------------


## Publisher Config: ---------------------------------------------------------------------------------------------------------------
  publisher {
    # this setting controls
    #     1) maximum size of the queue of published documents waiting to be processed
    #     2) maximum size of the queue of completed documents waiting to be indexed
    # e.g. if it is set to 10 then each each queue can contain up to 10 documents for a total of 20
    #
    # attempts to publish beyond this capacity will cause the publisher to block
    # this setting applies only when running Lucille in local / single-JVM mode using a LocalMessageManager
    # this setting defaults to 100
    queueCapacity: 100
  }
## ---------------------------------------------------------------------------------------------------------------------------------


## Pipeline(s) Config: ----------------------------------------------------------------------------------------------------------------
  pipelines: [{name: "pipeline1", stages: [{class: "com.kmwllc.lucille.stage.CreateChildrenStage"}]},
              {name: "pipeline2", stages: [{class: "com.kmwllc.lucille.stage.CreateChildrenStage"}]}
  ]

  # alternative syntax for declaring pipelines:
  pipelines: [{name: "pipeline1", stages: [{class: "com.kmwllc.lucille.stage.CreateChildrenStage"}]}]
  pipelines: ${pipelines} [{name: "pipeline2", stages: [{class: "com.kmwllc.lucille.stage.CreateChildrenStage"}]}]
## ---------------------------------------------------------------------------------------------------------------------------------


## Indexer Config: -----------------------------------------------------------------------------------------------------------------
  indexer {
      type: "Solr" # if omitted, defaults to Solr
      batchTimeout: 6000
      batchSize: 100
      idOverrideField: "identification"
      indexOverrideField: "new_index_field"
      ignoreFields: ["ignore_this_field1", "ignore_this_field2"]
      # at the moment, following settings are specific to Solr Indexer
      deletionMarkerField: "is_deleted"
      deletionMarkerFieldValue: "true"
      deleteByFieldField: "field_to_delete"
      deleteByFieldValue: "false"
  }

  # elastic search
  elastic {
    url: "http://localhost:9200"
    index: "lucille-default"
    type: "lucille-type"
    sendEnabled: false
  }

  # solr basic indexer config
  solr {
    url: "http://localhost:8983/solr/callbacktest"
  }

  # solr cloud mode with url
  solr {
    useCloudClient: true
    defaultCollection: "zktest2"
    url: ["http://localhost:8983/solr"]
  }

  # solr cloud mode with zkHosts
  solr {
    useCloudClient: true
    defaultCollection: "gettingstarted"
    zkHosts: ["zookeeper1:2181", "zookeeper2:2181", "zookeeper3:2181"]
    # optional
    zkChroot: "/solr"
  }

  # zookeeper settings for solr indexer
  zookeeper {
    connectString: "localhost:2181"
  }

  # open search
  opensearch {
    url: "https://admin:admin@localhost:9200"
    url: ${?OPENSEARCH_URL} # allow env override
    index: "lucille-default"
    index: ${?OPENSEARCH_INDEX}
    acceptInvalidCert: false # only enable for testing ssl/https against localhost
  }
## ----------------------------------------------------------------------------------------------------------------------------------


## Worker Config: -----------------------------------------------------------------------------------------------------------------
  worker {
    # maximum number of times across all workers that an attempt should be made to process any given document;
    # when this property is not provided, retries will not be tracked and no limit will be imposed;
    # this property requires that zookeeper is available at the specified zookeeper.connectString
    maxRetries: 2

    # number of worker threads to start for each pipeline when running lucille in local / single-JVM mode
    threads: 2

    # maximum time to spend processing a message, after which the worker will assume a problem and shut down
    # NOTE: not supported while com.kmwllc.lucille.core.Worker and PipelineWorker are being reconciled
    maxProcessingSecs: 600 # 10 minutes

    # name of pipeline
    pipeline: "pipeline_name"
  }
## ----------------------------------------------------------------------------------------------------------------------------------


## Kafka Config: -----------------------------------------------------------------------------------------------------------------
  kafka {
    bootstrapServers: "localhost:9092"

    # how long a kafka poll should wait for data before returning an empty record set
    pollIntervalMs: 250

    # maximum time allowed between kafka polls before consumer is evicted from consumer group
    maxPollIntervalSecs: 600 # 10 minutes

    # ID of consumer group that all lucille workers should belong to
    consumerGroupId: "lucille_workers"

    maxRequestSize: 250000000

    securityProtocol: "SSL"

    consumerPropertyFile: ".../consumer-conf/consumer.properties"

    producerPropertyFile: ".../consumer-conf/producer.properties"

    documentDeserializer: # TODO

    documentSerializer: # TODO

    events: # TODO

    sourceTopic: # TODO

    adminPropertyFile: ".../admin-conf/admin.properties"
  }
## ----------------------------------------------------------------------------------------------------------------------------------


## Misc: -----------------------------------------------------------------------------------------------------------------
  # TODO: Do we still need this dev block and relevant features?
  dev {
    kafka {
      bootstrapServers: "localhost:9092"
    }

    elastic {
      host: "localhost",
      port: "9200"
      sendEnabled: ${?ELASTIC_SEND_ENABLED}
    }
  }

  # for the VFSConnector
  aws {
    accessKeyId: "${?AWS_ACCESS_KEY_ID}",
    secretAccessKey: "${?AWS_SECRET_ACCESS_KEY}",
    defaultRegion: "${?AWS_DEFAULT_REGION}"
  }


  # file-to-file-example.conf illustrates how to reference other config files within a config file

  log {
    seconds: 30 # how often components (Publisher, Worker, Indexer) should log a status update
  }
## ----------------------------------------------------------------------------------------------------------------------------------

