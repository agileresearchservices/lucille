# this ingest runs two connectors in sequence
#   the first connector reads from a csv file (source.csv)
#   the second connector reads from a newline-delimited json file (source.ndjson)
#
# both connectors feed to the same simple pipeline that renames various fields by adding a "my_" prefix
#
# a CSV indexer appends all processed documents to a dest.csv file
#
# overall, what's happening is that data from one CSV file and one JSON file is being lightly transformed and
# consolidated into a single CSV
#
# this example illustrates how Lucille can operate in a file-to-file mode without actually interacting with a search backend
# this example illustrates how Lucille config files can be subdivided into smaller reusable sections that can be
# included from the main config


# this illustrates how we can include connector definitions from other files and bind them to variables
csv_connector = { include "csv-connector.conf" }
json_connector = { include "json-connector.conf"}

# this illustrates how we reference included connector definitions in the context of a connector list
connectors: [
    ${csv_connector},
    ${json_connector}
]

# this illustrates how we can include a list of pipelines defined in another file
# the one pipeline in that list is called "simple_pipeline" and is used by the connectors included above
include "simple-pipelines.conf"

kafka {
  bootstrapServers: "kafka:9092"

  # how long a kafka poll should wait for data before returning an empty record set
  pollIntervalMs: 250

  # maximum time allowed between kafka polls before consumer is evicted from consumer group
  maxPollIntervalSecs: 600 # 10 minutes

  # ID of consumer group that all lucille workers should belong to
  consumerGroupId: "lucille_workers"

  maxRequestSize: 250000000

  #securityProtocol: "SSL"

  # does not need to be set explicitly, unless you need to use a custom Deserializer
  documentDeserializer: "com.kmwllc.lucille.message.KafkaDocumentDeserializer"

  # does not need to be set explicitly, unless you need to use a custom Serializer
  documentSerializer: "com.kmwllc.lucille.message.KafkaDocumentSerializer"

  # if set to false, will not send Document failures / successes as messages to a Kafka event topic
  events: true

  # can set custom Kafka topic name which contains documents to be processed
  #sourceTopic: "pipeline1_source"

}

indexer {
  type: "csv"
}

# settings for specific indexer types are currently defined in dedicated blocks outside the indexer block itself
# TODO: move all indexer-related configuration under the indexer block
solr {
  useCloudClient: true
  defaultCollection: "quickstart"
  url: ["http://solr:8983/solr"]
}

csv {
  # names of columns to include in output csv; must match field names of documents output by the pipeline
  columns: ["my_name","my_country","my_price"]

  # path to output csv file
  path: "output/dest.csv"

  # says to append to output file rather than overwriting it if it already exists
  # needed here because two connectors are writing to the same file in sequence
  append: true

  # says to not write a header to the output file with the names of the CSV columns
  includeHeader: false
}
